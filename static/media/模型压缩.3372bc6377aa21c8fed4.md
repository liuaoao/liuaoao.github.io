# 模型压缩

本教程介绍了模型压缩的三大方法：**剪枝、蒸馏、量化**，最终在pc端完成部署。这些方法均采用各领域的经典思路来实现，剪枝部分（BN剪枝、直接减通道）、蒸馏部分（参考Hinton论文）、量化（MNN离线量化）。教程的目的是帮助自己快速在模型压缩领域入门，通过实验体会剪枝、蒸馏、量化的效果，并在模型调试方面积累经验。

[项目完整Code](https://link.zhihu.com/?target=https://github.com/tangchen2/Model-Compression) （欢迎各位Star ）

**模型压缩核心**：先缩小模型尺寸，再补偿下降的精度

实验模型：VGG16，ResNet34

数据集：CIFAR10

## 剪枝

**1. 直接减通道数**以VGG16为例，直接减通道的剪枝策略直接在每一层上将其通道数减半或减1/4，相当于按照VGG16的结构重新搭建了一个通道数更少的模型，这类模型直接初始化训练即可。该方案严格上应该不属于剪枝的范畴，只是仿照一个较好的模型结构，去减少通道数。一般来说适用于模型较大，任务相对比较简单的场景中，大模型应用在较简单的场景中往往有较多的参数冗余，因此直接采用减通道的方法保留模型结构，快速缩小模型次尺寸。

**2. BN剪枝**通常来说，模型会采用Conv+BN的组合，此时BN层的权重可以看作是筛选卷积层通道的重要因子。BN层中某一通道的权重越小，相当于卷积层对应通道的作用越小（卷积层某一通道乘以BN层对应通道的小权重，其输出接近于0，相当于该通道对应的输出为0，因此可以把这些通道视为不重要的通道，通过剪枝剪掉）**核心**：在BN层中找到权重较小的通道，对应的在卷积层中把这些通道剪掉，达到保留模型结构，缩小模型尺寸的目的。**问题**：常规训练的模型其BN的权重往往不会太接近于0，或者说都比较大，这种情况下，直接通过把权重相对较小的通道剪掉对模型效果的影响会比较大**解决方法**：**稀疏化训练**，在模型的常规损失函数上加上BN参数的正则化损失，强迫BN参数在训练过程中向0靠拢，训练完成后，把最接近于0的这部分BN参数对应的通道剪掉，即等价于把最不重要的通道剪掉。

### 1. VGG实验

- **直接减通道**

![https://pic1.zhimg.com/80/v2-fb870068a43d67043c6f1e77c46c0390_720w.jpg](https://pic1.zhimg.com/80/v2-fb870068a43d67043c6f1e77c46c0390_720w.jpg)

注：VGG16是原始结构，VGG16-HALF把所有的通道数减半，VGG16-QUARTER把所有的通道数减为1/4

通道数减为1/2的情况下，参数量降低了4倍，速递提升了2倍左右，精度掉了2%

通道数减为1/4的情况下，参数量降低了16倍，推理速度提升了4倍左右，精度掉了5%

对于上述两种直接减通道的方案，在模型参数量和推理速度的收益还是很明显的，对于丢掉的精度，下面会通过模型蒸馏来补偿。

- **BN剪枝**

VGG模型的结构比较简单，不涉及shortcut连接，且是conv+bn的标准结构，BN剪枝比较方便，BN层决定前面Conv层的剪枝，所有的conv+bn组合都可以参与剪枝

![https://pic4.zhimg.com/80/v2-fae3a305511fbdbbc1131905968cd373_720w.png](https://pic4.zhimg.com/80/v2-fae3a305511fbdbbc1131905968cd373_720w.png)

**稀疏化训练的重要性**：未进行稀疏化训练的VGG16在应用BN剪枝时，由于BN层的权重都比较大，剪掉的这些通道可能都是对模型推理有用的通道，因此精度掉的特别多，仅在裁剪率30%的情况下，精度就下降到只有32.78% 。

稀疏化训练的核心是**找到合适的scale**，scale太小，BN权重的正则化效果不明显，不能把权重压缩到0附近，影响后期的剪枝效果；scale太大，模型的总loss太大，训练难度增大（震荡）

**TODO**：代码中存在一个bug，若阈值设置不当，可能会有某一层通道减为0的情况，会报错，解决可以是设置一个阈值上限（每一层BN最大值中的最小值）thresh不能高于该值，否则会有某一层通道减为0的错误。

**TIPS**：可以把prune ratio调高一点，即使精度降的比较多，因为后期有蒸馏可以尝试提精度，所以这里调高prune ratio着重减小模型尺寸，当然也可以像上面这样ratio调成0.7，模型仍能保持很好的精度

### 2. ResNet实验

**注意**：ResNet34结构略有调整，CIFAR10数据集的尺寸只有32*32，原始的ResNet下采样尺寸太多，因此这里做了修改，模型相对ImageNet上的ResNet34要小了很多

### **剪枝策略**：

ResNet的剪枝策略较为复杂

1. repo中提供的ResNet模型是基于V2结构的，卷积块结构为BN - Relu - Conv，每个卷积块内部的Conv通道的剪枝由其前面的BN层决定，对比VGG Conv - BN - Relu，其Conv通道的剪枝由后面的BN层决定
2. ResNet存在shortcut结构，**bottleneck的最后一个Conv是不会剪枝的**，因为要确保和支路的通道数一致（最后合并是add操作）
3. **Channel Selection作用**，内部有一个index通过置0或置1可以筛选通道，放在第一个BN层之后，第一个BN层不会真正的减通道（通道数仍为256或512）只是Forward时由于channel selection中index的选择作用，会把一些通道屏蔽掉（伪剪枝，通道数并没有真正剪掉，但这些通道不起作用了）**Channel Selection在ResNet的剪枝中是必须的**，因为卷积块的结构式BN - Relu - Conv，第一个单独的卷积由于前面没有BN是剪枝的，此时后面的BottleNeck的第一个BN如果按照权重剪枝，那么这两层之间的通道数就不对了，没法串接起来了。所以BottleNeck的第一个BN只能通过Channel Selection进行**伪剪枝**（通道数不剪，权重太小的通道被屏蔽了）此外，BottleNeck之间可能会有一些单独的卷积相连，这些相连的卷积由于缺少关联的BN层也是无法剪枝的，因此后一个BottleNeck的第一个BN层需要Channel Selection确保和前面Conv层的维度一致
- **BN剪枝**

![https://pic4.zhimg.com/80/v2-74649a3fa42fe0862f69fc84a9837d07_720w.jpg](https://pic4.zhimg.com/80/v2-74649a3fa42fe0862f69fc84a9837d07_720w.jpg)

prune ratio为0.5时参数量变为原来的1/2，准确率几乎不变；prune ratio为0.7时，参数量变为原来的1/3，准确率下降严重。

前面也提到，剪枝阶段的核心目的是确保模型尺寸尽可能的小，对于prune ratio=0.5这种情况，既缩小模型尺寸，又保持精度固然很好，如果想进一步缩小模型尺寸，可以考虑prune ratio=0.7的方案，虽然精度掉的很严重，后续的蒸馏阶段成功的把精度补回来了。实际使用时考虑prune ratio 0.5和0.7两种方案共同操作，防止prune ratio=0.7的模型后期蒸馏精度提不回来，就采用prune ratio为0.5的方案。

## 蒸馏

提升剪枝模型精度的方法有两种

1. 在剪枝模型的基础上finetune，相当于继续微调参数，寻找最佳状态 => VGG16 - prune 0.7的finetune
2. 蒸馏，用一个训练好的大模型蒸馏剪枝后的模型，相当于更高级的finetune，将大模型的dark knowledge蒸给小模型 => VGG16 - QUARTER的蒸馏，ResNet(depth=92)蒸ResNet34(prune0.5/0.7)

**区别**：一般来说蒸馏模型的精度不会高于teacher model的精度，finetune的话可能会超过原始未剪枝模型的精度

注：蒸馏的大模型一般用同类型的网络效果较好，如resnet34用resnet151蒸馏，不同模型的混合蒸馏可能效果不一定好。

### 1. VGG蒸馏

- **Prune Finetune**

VGG16 - prune 0.7 由于和原始模型的VGG16 - Sparse Train精度相差不是很大，蒸馏可能效果一般，直接尝试finetune

![https://pic2.zhimg.com/80/v2-6e10e8bb946e3ca4327567f21d15c881_720w.png](https://pic2.zhimg.com/80/v2-6e10e8bb946e3ca4327567f21d15c881_720w.png)

Finetune效果不明显，可能是本身精度就比较高（接近达到VGG16 Origin的饱和精度）

- **蒸馏实验**

VGG16 - QUARTER 用VGG16去蒸馏，两者精度相差5%，但VGG16 - QUARTER的参数更少、速度更高，因此尝试通过蒸馏提升该模型的精度。

**方法**：采用Hinton最经典的Knowledge Distillation模型，loss = hard loss + soft loss，其中hard loss是student model和label的交叉熵，soft loss是student model和teacher model的预测输出分布的KL散度

![https://pic1.zhimg.com/80/v2-20ae070ab55b365d44c713ce57a9a76c_720w.jpg](https://pic1.zhimg.com/80/v2-20ae070ab55b365d44c713ce57a9a76c_720w.jpg)

蒸馏后，以VGG16 - QUARTER - KD（alpha=0.9, lr=0.01, temperature=10）为例，效果提升约2%，仍然没有BN剪枝好，对比BN剪枝模型的参数量（923K vs 1.17M）可能是参数量的原因，VGG16 - QUARTER的效果达到了当前参数的瓶颈（猜测）

### 2. ResNet蒸馏

ResNet的蒸馏针对ratio=0.5和0.7分别做了实验，对0.5做纯粹是为了拉高精度（锦上添花），对0.7做是为了证明前面的TIPS（剪枝部分先重点把模型结构缩小，降低的精度可以通过蒸馏弥补）两者进行对比。

![https://pic4.zhimg.com/80/v2-bce78e948c51b61000e1701904813253_720w.jpg](https://pic4.zhimg.com/80/v2-bce78e948c51b61000e1701904813253_720w.jpg)

ResNet部分蒸馏的效果还是比较明显的，ResNet34剪枝模型在蒸馏后精度逼近ResNet90大模型，超越了原始的ResNet34模型。

### **蒸馏体会**

1. learning rate。蒸馏时learning rate调太小，模型效果基本不提升，一直在原精度附近徘徊，涨不上去，learning rate稍稍调大一些，模型在前期精度会下降，但是后期学习率衰减之后会补偿回来（**不要太在意前期精度的下降**）=> 一般倾向于learning rate调大一点，调小了可能训练整个过程不起作用。
2. 温度temperature。影响teacher model给出的软分布信息，这个信息后续是直接指导student model训练的，因此temperature的取值会直接影响模型的蒸馏效果
    
    TODO：Temperature=50正在测试
    
3. loss的比值alpha，该项是决定soft loss（teacher model对student的指导）占比的，一般先观察soft loss和hard loss的大小关系，选择适当比例

## 部署

部署基于**MNN框架**实现

### 卷积BN融合

作用：加快网络推理速度，本来模型推理时conv、bn两层，要做两次计算，在推理时把bn层融合到卷积层中，只需一次计算。

原理：推理时BN层的参数是固定的，此时该层可以看成一个卷积层，融合是指通过参数的等效变换，把bn层的参数乘加到前一conv层的参数中，后面的BN层可以去掉 => 网络整体的计算量变小了，BN层的计算全都省略了，这部分的参数移到前面的卷积层了 => 变换后的卷积层其参数相当于完成了卷积 + BN两件事。

**上述所有实验（VGG + ResNet）在剪枝之后就已经完成了卷积BN融合。**

MNN模型转化：MNNConvert将.onnx格式的模型转化成.mnn格式，后续部署直接在C++中完成

**推理流程**：读取图片 => 预处理（RGB转换 + 归一化）=> MNN调模型 => 获取模型输出

MakeFile编译推理.cpp文件得到可执行文件，完成部署

## 量化

量化基于**MNN框架**实现，直接采用MNN自带的Quantized完成

理论核心：**确定合适的尺度因子**，使fp32和int8之间的信息损失最小 => 如何确定？最优化问题

1. KL散度（fp32分布和int8分布尽可能一致）
2. L2损失（fp32 => int8 => fp32，和原始的fp32尽可能接近）

**内部实现**：量化过程中，遍历所有Op，若该Op支持量化，则将该Op量化；若该Op不支持量化，如果其输入是量化Op给的，后接反量化模块，将量化的输出变回未量化的输出；如果其输出后续给到量化Op，则输出后接量化模块，将该Op未量化的输出变成量化的输出。所以整个模型经过量化之后，内部支持量化的Op其参数是量化之后的int8精度，不支持量化的Op其参数仍是原始的float32精度，如果涉及量化和未量化Op相连，则要插入相应的量化 / 反量化模块

MNN量化要点：**quant_config.json**配置量化预处理操作（图像归一化参数mean, std，校准图像目录，量化方法）

- **ResNet量化**

模型 ResNet34 - Sparse Train - prune0.5（alpha=0.9, lr=0.01, temperature=3）用作量化示范

![https://pic1.zhimg.com/80/v2-3d072e163fa9f253f96a82ecaa386f44_720w.png](https://pic1.zhimg.com/80/v2-3d072e163fa9f253f96a82ecaa386f44_720w.png)

量化后模型尺寸明显缩小，926KB => 315KB，但在mac上测试时发现量化后的模型反而推理耗时增加了（耗时增加查下来可能是因为mac的cpu不支持int8这类定点运算，导致量化后的int8模型实际测试时还是会转化为64位，这个转化导致耗时增加 <= 不确定是不是这个原因，但自己测试了好几个模型量化之后耗时都增加了）

量化后模型的精度下降了约2%，精度损失感觉还是挺严重的，量化配置中校准图片选择测试集图片，数量无论是100张或600张或2000张，量化后的精度基本都在89.5%～90.0%之间浮动，mean和normal都和之前一致。**不知道这个精度下降的点数是否正常，若不正常，暂时未找到原因**。

## 致谢

### 剪枝

[ResNet剪枝](https://zhuanlan.zhihu.com/p/101690057)

### 蒸馏

[Distilling the Knowledge in a Neural Network](https://link.zhihu.com/?target=https://arxiv.org/pdf/1503.02531.pdf)

### 量化

[MNN量化原理](https://zhuanlan.zhihu.com/p/81243626)

[MNN量化实现](https://zhuanlan.zhihu.com/p/153562409)

### 部署

[MNN框架](https://link.zhihu.com/?target=https://www.yuque.com/mnn/cn)

[卷积BN融合](https://zhuanlan.zhihu.com/p/110552861)